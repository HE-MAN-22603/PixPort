#!/usr/bin/env python3
"""
PixPort - AI Passport Photo Maker
Optimized for Google Cloud Run with preloaded AI models
"""

# Set numba environment variables early to prevent cache issues
import os
os.environ.setdefault('NUMBA_DISABLE_JIT', '1')
os.environ.setdefault('NUMBA_CACHE_DIR', '/tmp/.numba_cache')
os.environ.setdefault('NUMBA_DISABLE_PERFORMANCE_WARNINGS', '1')

from app import create_app
import logging
import time

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Detect deployment environment
is_cloud_run = os.environ.get('K_SERVICE') is not None
is_railway = os.environ.get('RAILWAY_ENVIRONMENT_NAME') is not None
is_flyio = os.environ.get('FLY_APP_NAME') is not None or os.environ.get('FLY_DEPLOYMENT') is not None

if is_flyio:
    logger.info('ü™Ç Starting PixPort on Fly.io')
    # Force isnet-general-use for Fly.io free plan
    os.environ.setdefault('REMBG_MODEL', 'isnet-general-use')
    os.environ.setdefault('MEMORY_CONSTRAINED', 'true')
elif is_cloud_run:
    logger.info('üöÄ Starting PixPort on Google Cloud Run')
elif is_railway:
    logger.info('üöÑ Starting PixPort on Railway')
else:
    logger.info('üíª Starting PixPort in local development mode')

app = create_app()

# ==========================================
# üöÄ OPTIMIZED MODEL PRELOADING FOR CLOUD RUN
# ==========================================

def preload_ai_models():
    """
    Preload AI models during container startup for faster first requests.
    This eliminates the 10-15 second delay on first background removal.
    """
    start_time = time.time()
    
    try:
        logger.info("üì¶ Preloading AI models for optimal Cloud Run performance...")
        
        # Import our optimized model manager
        from model_utils import load_model
        
        # Preload the model during startup
        success = load_model()
        
        if success:
            preload_time = time.time() - start_time
            logger.info(f"‚úÖ AI model preloaded in {preload_time:.2f}s - first requests will be fast!")
            return True
        else:
            logger.warning("‚ö†Ô∏è Model preload failed - models will load on first request")
            return False
            
    except Exception as e:
        load_time = time.time() - start_time
        logger.error(f"‚ùå Model preload failed after {load_time:.2f}s: {e}")
        logger.info("üîÑ Falling back to on-demand model loading")
        return False

# Preload models based on environment
if is_cloud_run or (not is_railway and not os.environ.get('SKIP_AI_MODELS')):
    # Preload on Cloud Run and local development
    logger.info("üéØ Environment supports model preloading")
    preload_ai_models()
else:
    # Skip preload on Railway (memory constraints) or when explicitly disabled
    logger.info("‚è© Skipping model preload - using on-demand loading")

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_ENV') == 'development'
    
    # Print startup information
    print(f'\nüöÄ Starting PixPort Server on port {port}')
    print(f'üìä Debug mode: {"ON" if debug else "OFF"}')
    print(f'üåç Environment: {"Production" if os.environ.get("RAILWAY_ENVIRONMENT_NAME") else "Development"}')
    print(f'üîó Local URL: http://127.0.0.1:{port}')
    print(f'üîó Network URL: http://0.0.0.0:{port}')
    print('\n‚úÖ Server ready to accept connections!')
    print('='*50)
    
    app.run(host='0.0.0.0', port=port, debug=debug)
